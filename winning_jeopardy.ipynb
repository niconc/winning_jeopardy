{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Winning on Jeopardy!\n",
    "**Let's say you want to compete on Jeopardy, and you're looking for any edge you can get to win. In this project, we'll work with a dataset of Jeopardy questions to figure out some patterns in the questions that could help someone to win.**\n",
    "\n",
    "The dataset is named jeopardy.csv, and it's a subset containing 20000 rows (from around 220.000) from the beginning of a full dataset of Jeopardy questions.\n",
    "\n",
    "Let's begin by reading the file and do some initial stuff necessary to the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "jeopardy = pd.read_csv(\"jeopardy.csv\") # read the dataset.\n",
    "jeopardy.head() # display the first 5 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(jeopardy.columns) # print out the columns.\n",
    "\n",
    "# clear whitespace on column names.\n",
    "jeopardy.columns = jeopardy.columns.str.strip()\n",
    "\n",
    "print(jeopardy.columns) # print columns again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize the Question/Answer columns\n",
    "The idea is to ensure that we lowercase words and remove punctuation so Don't and don't aren't considered to be different words when you compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "def normalize_text(text_str):\n",
    "    text_str = text_str.lower() # lowercase the string\n",
    "    # remove non-word characters (punctuation symbols)\n",
    "    text_str = text_str.replace(r\"[^\\w\\s]+\", '')  \n",
    "    return text_str\n",
    "\n",
    "# Perform normalization.\n",
    "# Apply the function to each item in the Question column \n",
    "# and assign the result to the clean_question column (a new column).\n",
    "jeopardy[\"clean_question\"] = jeopardy[\"Question\"].apply(normalize_text)\n",
    "\n",
    "# Apply the function to each item in the Answer column \n",
    "# and assign the result to the clean_answer column (a new column).\n",
    "jeopardy[\"clean_answer\"] = jeopardy[\"Answer\"].apply(normalize_text)\n",
    "\n",
    "jeopardy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize the Value & Air Date columns.\n",
    "The Value column should also be numeric, to allow us to manipulate it more easily. We'll need to remove the dollar sign from the beginning of each value and the \",\" thousand separator and convert the column from text to numeric.\n",
    "\n",
    "The Air Date column should also be a datetime, not a string, to enable us to work with it more easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a second, specialized normalization function.\n",
    "def normalize_text_2(text_str):\n",
    "    # Remove the \"$\" character in front of the value.\n",
    "    text_str = text_str.replace(\"$\", '')\n",
    "    \n",
    "    # Remove the \",\" character (thousand separator) from the value.\n",
    "    text_str = text_str.replace(\",\", '') \n",
    "    \n",
    "    # Convert the string to integer...\n",
    "    try:\n",
    "        to_integer = int(text_str)\n",
    "    # ... in failure, assign 0.\n",
    "    except:\n",
    "        to_integer = 0\n",
    "    \n",
    "    return to_integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Normalize the Value column.\n",
    "# Apply the function to each item in the Value column & assign\n",
    "# the result to the \"clean_value column\" (a new column).\n",
    "jeopardy[\"clean_value\"] = jeopardy[\"Value\"].apply(normalize_text_2)\n",
    "\n",
    "# Apply the function to convert the Air Date column to a datetime column.\n",
    "jeopardy[\"Air Date\"] = pd.to_datetime(jeopardy[\"Air Date\"])\n",
    "\n",
    "jeopardy.dtypes # Display the types to double check."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Study the data or not?\n",
    "In order to figure out whether to study past questions, study general knowledge, or not study it all, it would be helpful to figure out two things:\n",
    "\n",
    "**1. How often the answer is deducible from the question.**\n",
    "\n",
    "**2. How often new questions are repeats of older questions.**\n",
    "\n",
    "We can answer the second question by seeing how often complex words (> 6 characters) reoccur.\n",
    "We can answer the first question by seeing how many times words in the answer also occur in the question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Question #1: How often the answer is deducible from the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define a function that gets a row from jeopardy and:\n",
    "def count_matches(row):\n",
    "    # Split the clean_answer column around spaces\n",
    "    split_answer = row[\"clean_answer\"].split(\" \")\n",
    "\n",
    "    # Split the clean_question column around spaces\n",
    "    split_question = row[\"clean_question\"].split(\" \")\n",
    "    \n",
    "    match_count = 0\n",
    "    \n",
    "    # Remove the \"the\" article from the answer.\n",
    "    split_answer = [elem for elem in split_answer if elem != \"the\"]\n",
    "\n",
    "    # If there's no answer at all, return with 0.\n",
    "    if len(split_answer) == 0:\n",
    "        return 0\n",
    "    \n",
    "    # Loop through each item in split_answer and see if it occurs\n",
    "    # in split_question. If it is, count it.\n",
    "    for item in split_answer:\n",
    "        if item in split_question: \n",
    "            match_count += 1       \n",
    "            \n",
    "    # return the mean of matched words occuring in question\n",
    "    # against the total length of the answer.\n",
    "    return match_count / len(split_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Apply the function in each row of Jeopardy and count how many times\n",
    "# terms in clean_answer occur in clean_question.\n",
    "# Create a new column with the result.\n",
    "jeopardy[\"answer_in_question\"] = jeopardy.apply(count_matches, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find the mean of the answer_in_question column.\n",
    "mean_answer_in_q = jeopardy[\"answer_in_question\"].mean()\n",
    "print(round(mean_answer_in_q * 100, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question #1 Answer.\n",
    "The answer only appears in the question **about 4.5% of the time.** This isn't a huge number, and means that we probably can't just hope that hearing a question will enable us to figure out the answer.\n",
    "\n",
    "So, the answer to **\"How often the answer is deducible from the question?\"** is that it rarely happens. \n",
    "\n",
    "#### Conclusion:\n",
    "We'll probably have to study."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question #2. How often new questions are repeats of older questions?\n",
    "We can't completely answer this, because you only have about 10% of the full Jeopardy question dataset, but we can investigate it at least."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "question_overlap = []\n",
    "terms_used = set()\n",
    "\n",
    "# Sort Jeopardy by ascending air date. (OLDER FIRST)\n",
    "jeopardy = jeopardy.sort_values(\"Air Date\", ascending=True)\n",
    "\n",
    "# Loop through each row of jeopardy.\n",
    "for idx, row in jeopardy.iterrows():\n",
    "    # Split the clean_question column of the row on the space character.\n",
    "    split_question = row[\"clean_question\"].split(\" \")\n",
    "    \n",
    "    # Remove any word shorter than 6 characters.\n",
    "    split_question = [r_words for r_words in split_question\n",
    "                      if len(r_words) >= 6]\n",
    "\n",
    "    match_count = 0\n",
    "    \n",
    "    # Loop through each word in split_question. If the term occurs\n",
    "    # already in terms_used, count it. \n",
    "    # (That means the term is repeated in the past)\n",
    "    for word in split_question:\n",
    "        if word in terms_used:\n",
    "            match_count += 1\n",
    "        else: \n",
    "        # if not, occurs in terms, add it.\n",
    "            terms_used.add(word)\n",
    "    \n",
    "    # If there are terms, find the mean of the terms against\n",
    "    # the total length of words greater than 6 chars\n",
    "    # (and that's because some terms are present more than once)\n",
    "    if len(split_question) > 0:\n",
    "        match_count /= len(split_question)\n",
    "        \n",
    "    # Add the mean just counted to the list\n",
    "    # and repeat the process to the next row of Jeopardy.\n",
    "    question_overlap.append(match_count)\n",
    "\n",
    "# Create a new column with the question_overlap list\n",
    "# and find the mean of it (the mean of all means).   \n",
    "jeopardy[\"question_overlap\"] = question_overlap\n",
    "print(\"Proportional mean:\", round(\n",
    "    jeopardy[\"question_overlap\"].mean() * 100, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question #2 Answer.\n",
    "We have 60% overlap between terms in new questions and terms in old questions. from this measurement, we can observe the following:\n",
    "\n",
    "* Don't forget we have a small set of the Jeopardy dataset (around 10%) .\n",
    "* The test was carried out against single words (terms) and not with phrases.\n",
    "\n",
    "#### Conclusion:\n",
    "\n",
    "Although we have limitations, it's worth to investigate it further. I'm having a feeling that Jeopardy, recycles questions over time, by (maybe) reforming them in one way or another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Focus on high value questions.\n",
    "Let's say you only want to study questions that pertain to high value questions instead of low value questions. This will help you earn more money when you're on Jeopardy.\n",
    "\n",
    "We can actually figure out which terms correspond to high-value questions using a chi-squared test. We'll first need to narrow down the questions into two categories:\n",
    "\n",
    "* Low value -- Any row where Value is less than 800.\n",
    "* High value -- Any row where Value is greater than 800."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# High/Low value function return. \n",
    "# We make the distinction based on > $800 or < $800\n",
    "def quest(row):\n",
    "    if row[\"clean_value\"] > 800: # if value is greater than 800\n",
    "        value = 1                # count it as \"High\" value\n",
    "    else:\n",
    "        value = 0                # else, count it as \"Low\"\n",
    "    return value\n",
    "\n",
    "# Apply the function to dataset and get a new column\n",
    "# with 1's (aka \"High\") or 0's (aka \"Low\")\n",
    "jeopardy[\"high_value\"] = jeopardy.apply(quest, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A function that counts the values.\n",
    "def quest_word(word):\n",
    "    low_count = 0\n",
    "    high_count = 0\n",
    "    \n",
    "    # Loops through each row in Jeopardy\n",
    "    for idx, row in jeopardy.iterrows():\n",
    "\n",
    "        # Split the question into words\n",
    "        sq = row[\"clean_question\"].split(\" \")\n",
    "        \n",
    "        # if the word passed into the func, contained in the question...\n",
    "        if word in sq:\n",
    "            # ... and if trhe row we're looping is a \"High\" row\n",
    "            if row[\"high_value\"] == 1:\n",
    "                high_count += 1 # count as \"High\".\n",
    "            else:\n",
    "                low_count += 1  # count as \"Low\".\n",
    "    # return the counters            \n",
    "    return high_count, low_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's do some testing by selecting 10 random elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Sample the terms set. Select 10 terms randomly\n",
    "comparison_terms = random.sample(terms_used, k=10)\n",
    "# chi-squared observed and expected values.\n",
    "observed_expected = []\n",
    "\n",
    "# Loop through each term and run the function on the term\n",
    "# to get the high value and low value counts.\n",
    "for term in comparison_terms:\n",
    "    result = quest_word(term)\n",
    "    observed_expected.append(result)\n",
    "\n",
    "# print the observed and expected values.\n",
    "print(observed_expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the expected counts and the chi-squared value.\n",
    "Now that we've found the observed counts for a few terms, we can compute the expected counts and the chi-squared value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import chisquare\n",
    "import numpy as np\n",
    "\n",
    "# Find the number of rows in jeopardy where high_value is 1.\n",
    "high_value_count = jeopardy[jeopardy[\"high_value\"] == 1].shape[0]\n",
    "\n",
    "# Find the number of rows in jeopardy where high_value is 0.\n",
    "low_value_count = jeopardy[jeopardy[\"high_value\"] == 0].shape[0]\n",
    "\n",
    "chi_squared = [] # This list will hold the results.\n",
    "# Loop through each list in observed_expected\n",
    "for obs in observed_expected:\n",
    "    # Add up both items in the list (high and low counts) \n",
    "    # to get the total count.\n",
    "    total = sum(obs)\n",
    "    \n",
    "    # Divide total by the number of rows in jeopardy\n",
    "    # to get the proportion across the dataset.\n",
    "    total_prop = total / jeopardy.shape[0]\n",
    "    \n",
    "    # Multiply total_prop by high_value_count to get \n",
    "    # the expected term count for high value rows.\n",
    "    high_value_exp = total_prop * high_value_count\n",
    "    \n",
    "    # Multiply total_prop by low_value_count to get \n",
    "    # the expected term count for low value rows.\n",
    "    low_value_exp = total_prop * low_value_count\n",
    "    \n",
    "    # Compute the chi-squared value and p-value \n",
    "    # given the expected and observed counts.\n",
    "    observed = np.array([obs[0], obs[1]])\n",
    "    expected = np.array([high_value_exp, low_value_exp])\n",
    "    chi_squared.append(chisquare(observed, expected))\n",
    "    \n",
    "chi_squared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chi-squared results\n",
    "None of the terms had a significant difference in usage between high value and low value rows. Additionally, the frequencies were all lower than 5, so the chi-squared test isn't as valid. It would be better to run this test with only terms that have higher frequencies."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
